\documentclass[12pt,a4paper]{article}
\usepackage[a4paper,margin=0.8in,footskip=0.25in]{geometry}

\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{mathtools}
\usepackage{physics}
\usepackage{stmaryrd}
\usepackage{amsthm}

\theoremstyle{remark}
\newtheorem{claim}{Claim}

\usepackage{enumerate}

\usepackage{fontspec} % 加這個就可以設定字體
\usepackage{xeCJK} % 讓中英文字體分開設置

\setCJKmainfont{標楷體} % 設定中文的字型，可以直接輸入系統裡有的字型
% \setmainfont{Times New Roman}

\newfontfamily{\Arial}{Arial}
\newfontfamily{\Calibri}{Calibri}
\newfontfamily{\Times}{Times New Roman}

\newCJKfontfamily\Kai{標楷體} % 定義指令\Kai則切換成標楷體
\newCJKfontfamily\Hei{微軟正黑體} % 定義指令\Hei則切換成正黑體
\newCJKfontfamily\NewMing{新細明體} % 定義指令\NewMing則切換成新細明體

\XeTeXlinebreaklocale "zh" % 這二行，中文才能自動換行
\XeTeXlinebreakskip = 0pt plus 1pt

\title{ML Foundation: HW3}
\author{b04902053 鄭淵仁}

\begin{document}
\maketitle
\section{} % 1
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{code/q1.png}
\end{figure}
\section{} % 2
\begin{claim}
	${H}^{2} = H$
	\begin{proof}
		\[
			\begin{aligned}
				{H}^{2} &= {(X {({X}^{T}X)}^{-1} {X}^{T})} ^ {2} \\
						&= (X {({X}^{T}X)}^{-1} {X}^{T}) (X {({X}^{T}X)}^{-1} {X}^{T}) \\
						&= X {({X}^{T}X)}^{-1} [ ({X}^{T} X) {({X}^{T}X)}^{-1} ] {X}^{T} \\
						&= X {({X}^{T}X)}^{-1} {X}^{T} \\
						&= H \\
			\end{aligned}
		\]
	\end{proof}
\end{claim}
With the claim above, we can prove that:
\begin{proof}
	\[
		\begin{aligned}
			{(I-H)}^{2} &= {I}^{2} - 2IH + {H}^{2} \\
						&= I - 2H + H \\
						&= I - H \\
		\end{aligned}
	\]
\end{proof}
\section{} % 3
\begin{proof}
	SGD with the error function given in the question:
	\[
		w_{t+1} \leftarrow w_{t} + \eta \cdot max(0, -yw_{t}^{T}x) (y_{n}x_{n})
	\]
	normal PLA:
	\[
		w_{t+1} \leftarrow w_{t} + \left\llbracket y_{n} \neq sign(w_{t}^{T}x_{n}) \right\rrbracket (y_{n}x_{n})
	\]
	\begin{enumerate}[{Case} 1{:}]
		\item $y = sign(w^{T}x)$
		\[
			\begin{gathered}
				-yw^{T}x < 0 \\
				max(0, -yw^{T}x) = 0 \\
				w_{t+1} \leftarrow w_{t} + 0
			\end{gathered}
		\]
		$w$ in SGD is not updated, which is the same as PLA.
		\item $y = -sign(w^{T}x)$
		\[
			\begin{gathered}
				-yw^{T}x > 0 \\
				max(0, -yw^{T}x) = -yw^{T}x \\
				w_{t+1} \leftarrow w_{t} + \eta \cdot (-yw_{t}^{T}x) (y_{n}x_{n})
			\end{gathered}
		\]
		$w$ in SGD is updated by $-yw^{T}x$. Therefore if $\eta = 1$ and $w_{t}^{T}x$ is large enough, this is also the same as PLA.
	\end{enumerate}
	Therefore, SGD with the $err(w)$ given results in PLA.
\end{proof}
\section{} % 4
\begin{proof}
	\[
		\hat{{E}_2}(\Delta u,\Delta v)
			= E(u, v) + \nabla E(u, v) \cdot (\Delta u, \Delta v)
			+ \frac{1}{2} {(\Delta u, \Delta v)}^{T} {\nabla}^{2}E(u, v) (\Delta u, \Delta v)
	\]
	Set the partial differences of $\hat{{E}_2}(\Delta u,\Delta v)$ be $0$, we have :
	\[
		\left\{
			\begin{aligned}
				0 = \pdv{\hat{{E}_2}(\Delta u,\Delta v)}{\Delta u}
					&= \pdv{E}{u} + \frac{1}{2}\left( 2 \pdv[2]{E}{u} \Delta u + 2 \pdv{E}{u}{v} \Delta v \right) \\
					&= \pdv{E}{u} + \pdv[2]{E}{u} \Delta u + \pdv{E}{u}{v} \Delta v \\
				0 = \pdv{\hat{{E}_2}(\Delta u,\Delta v)}{\Delta v}
					&= \pdv{E}{v} + \pdv[2]{E}{v} \Delta v + \pdv{E}{v}{u} \Delta u \\
			\end{aligned}
		\right.
	\]
	Simplify the equations :
	\[
		\left\{
			\begin{aligned}
				0 &= \pdv{E}{u} + \pdv[2]{E}{u} \Delta u + \pdv{E}{u}{v} \Delta v \\
				0 &= \pdv{E}{v} + \pdv[2]{E}{v} \Delta v + \pdv{E}{v}{u} \Delta u \\
			\end{aligned}
		\right.
	\]
	Now combine the two equations to one equation by vector $(u, v)$ :
	\[
		\begin{aligned}
			0 &= \nabla E(u, v) + {\nabla}^{2} E(u, v) \cdot (\Delta u, \Delta v) \\
			- {\nabla}^{2} E(u, v) \cdot (\Delta u, \Delta v) &= \nabla E(u, v) \\
			(\Delta u, \Delta v) &= - {\left({\nabla}^{2} E(u, v)\right)}^{-1} \nabla E(u, v)
		\end{aligned}
	\]
\end{proof}
\section{} % 5
\[
		\max_{h} \prod_{n=1}^{N} h_{y}\left(x_{n}\right) 
		= \max_{w} \prod_{n=1}^{N}
			\frac{\exp({w}_{y_{n}}^{T}x_{n})}{\sum_{k=1}^{K} \exp({w}_{k}^{T}x_{n})}
\]
Take natural log on it :
\[
	\begin{aligned}
		& \max_{w} \ln \prod_{n=1}^{N}
			\frac{\exp({w}_{y_{n}}^{T}x_{n})}{\sum_{k=1}^{K} \exp({w}_{k}^{T}x_{n})} \\
		&= \max_{w} \sum_{n=1}^{N} \ln
			\frac{\exp({w}_{y_{n}}^{T}x_{n})}{\sum_{k=1}^{K} \exp({w}_{k}^{T}x_{n})} \\
		&= \max_{w} \sum_{n=1}^{N} \left( \ln( \exp({w}_{y_{n}}^{T}x_{n}) ) -
			\ln\sum_{k=1}^{K} \exp({w}_{k}^{T}x_{n}) \right) \\
		&= \min_{w} \sum_{n=1}^{N} \left( \ln\sum_{k=1}^{K} \exp({w}_{k}^{T}x_{n}) -
			{w}_{y_{n}}^{T}x_{n} \right) \\
	\end{aligned}
\]
Therefore the $E_{in}$ is :
\[
	E_{in} = \frac{1}{N} \sum_{n=1}^{N} \left( \ln \sum_{k=1}^{K} \exp({w}_{k}^{T}x_{n}) -
		{w}_{y_{n}}^{T}x_{n} \right) \\
\]
\section{} % 6
First compute :
\[
	\begin{aligned}
		& \pdv{\left(\sum_{n=1}^{N} \left( \ln\sum_{k=1}^{K}
			\exp({w}_{k}^{T} x_{n})\right)\right)}{w_{i}} \\
		&= \sum_{n=1}^{N} \left( \frac{\exp({w}_{i}^{T} x_{n})}
			{\sum_{k=1}^{K} \exp({w}_{k}^{T} x_{n})}x_{n} \right) \\
		&= \sum_{n=1}^{N} \left( h_{i}(x_{n}) x_{n} \right) \\
	\end{aligned}
\]
Therefore the answer is :
\[
	\begin{aligned}
		\pdv{E_{in}}{w_{i}}
			&= \pdv{\left(\frac{1}{N} \sum_{n=1}^{N} \left( \ln \sum_{k=1}^{K} \exp({w}_{k}^{T}x_{n}) -
			{w}_{y_{n}}^{T}x_{n} \right) \right)}{w_{i}} \\
		&= \frac{1}{N} \sum_{n=1}^{N} \left( \left( h_{i}(x_{n}) x_{n} \right) -
			[[y_{n}=i]] x_{n} \right) \\
		&= \frac{1}{N} \sum_{n=1}^{N} \left( \left( \left( h_{i}(x_{n}) \right) -
			[[y_{n}=i]] \right) x_{n} \right) \\
	\end{aligned}
\]
\section{} % 7
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{code/q7.png}
	\caption{Histogram of ${E}_{out}$}
	\label{fig:q7}
\end{figure}
Figure \ref{fig:q7} shows the histogram of ${E}_{out}$.
\section{} % 8
\begin{figure}[h]%
	\begin{subfigure}[h]{0.45\textwidth}
		\includegraphics[width=\textwidth]{code/q8_001.png}
		\caption{$lr=0.001$}
	\end{subfigure}
	\hfill\vrule\hfill
	\begin{subfigure}[h]{0.45\textwidth}
		\includegraphics[width=\textwidth]{code/q8_01.png}
		\caption{$lr=0.01$}
	\end{subfigure}%
	\caption{Comparison between $GD$ and $SGD$ in $E_{in}$.}
	\label{fig:q8}
\end{figure}
從上面兩張圖中，我發現有以下三點現象：
\begin{enumerate}
	\item (GD 和 SGD 的差異) 我發現 GD 的 $E_{in}$ 很快就會穩定不變，或是穩定下降，而不會上下亂跳；相較之下， SGD 的 $E_{in}$ 則是很容易上下浮動。 \\
	我想這是因為 SGD 一次只會取一筆資料來計算 gradient ，如果這一筆資料有 noise 的話，算出來的 gradient 很容易會被這個 noise 影響；相較之下， GD 一次會用所有資料來計算 gradient ，所以算出來的結果一定會讓所有 training data 的 $E_{in}$ 變小或幾乎不變。
	\item ($lr = 0.001$ 和 $lr = 0.01$ 的差異) 我發現無論是 GD 或 SGD ， $lr = 0.001$ 的時候， $E_{in}$ 除了一開始上下亂跳以外，接下來就幾乎固定在 $0.46$ 左右了；而 $lr = 0.01$ 的時候， $E_{in}$ 會一直下降到 $0.21$ 。 \\
	我想這是因為 $0.001$ 的 learning rate 太小了，下降的速度太慢，甚至很容易卡在局部極值出不去；而 $0.01$ 的 learning rate 則是比較洽當的值，所以 $E_{in}$ 才能一直下降。
	\item 我發現到最後 GD 和 SGD 收斂到很接近的值。\\
	我想這是因為以期望值而言， SGD 算出來的 gradient 和 GD 的 gradient 會是一樣的，但是 SGD 會有 noise ，所以一開始 GD 和 SGD 的結果會差很多。但是夠多的 iteration 之後， SGD 的 $E_{in}$ 會因為跑過所有的點很多次，而且有相同的 learning rate，所以有比較高的機率可以到相同的極值。
\end{enumerate}
\section{} % 9
\begin{figure}[h]%
	\begin{subfigure}[h]{0.45\textwidth}
		\includegraphics[width=\textwidth]{code/q9_001.png}
		\caption{$lr=0.001$}
	\end{subfigure}
	\hfill\vrule\hfill
	\begin{subfigure}[h]{0.45\textwidth}
		\includegraphics[width=\textwidth]{code/q9_01.png}
		\caption{$lr=0.01$}
	\end{subfigure}%
	\caption{Comparison between GD and SGD in $E_{out}$.}
	\label{fig:q9}
從上面兩張圖中，我發現有以下 5 點現象：
\begin{enumerate}
	\item $E_{out}$ 的結果和 $E_{in}$ 的結果很像。\\
	我想這是因為 $E_{out}$ 和 $E_{in}$ 的 noise 沒有太多，而且取的資料量又都相對夠多，所以 $E_{out}$ 的結果才會和 $E_{in}$ 很像。
	\item (GD 和 SGD 的差異) 我發現 GD 的 $E_{in}$ 很快就會穩定不變，或是穩定下降，而不會上下亂跳；相較之下， SGD 的 $E_{in}$ 則是很容易上下浮動。 \\
	我想這個原因跟 $E_{in}$ 的這個現象的原因是有關係的：因為 SGD 一次只會取一筆資料來計算 gradient ，如果這一筆資料有 noise 的話，算出來的 gradient 很容易會被這個 noise 影響，所以 $E_{out}$ 的值也會被影響；相較之下， GD 一次會用所有資料來計算 gradient ，所以算出來的結果比較穩定，不會只受單一資料的 noise 影響，所以 $E_{out}$ 比較不會上下大幅變動。
	\item ($lr = 0.001$ 和 $lr = 0.01$ 的差異) 我發現無論是 GD 或 SGD ， $lr = 0.001$ 的時候， $E_{in}$ 除了一開始上下亂跳以外，接下來就幾乎固定在 $0.47$ 左右了；而 $lr = 0.01$ 的時候， $E_{in}$ 會一直下降到 $0.22$ 。 \\
	我想這個原因跟 $E_{in}$ 的這個現象的原因是一樣的：因為 $0.001$ 的 learning rate 太小了，下降的速度太慢，甚至很容易卡在局部極值出不去；而 $0.01$ 的 learning rate 則是比較洽當的值，所以 $E_{out}$ 才能一直下降。
	\item 我發現到最後 GD 和 SGD 收斂到很接近的值。\\
	我想這個原因跟 $E_{in}$ 的這個現象的原因是一樣的：因為以期望值而言， SGD 算出來的 gradient 和 GD 的 gradient 會是一樣的，但是 SGD 會有 noise ，所以一開始 GD 和 SGD 的結果會差很多。但是夠多的 iteration 之後， SGD 的 $E_{in}$ 會因為跑過所有的點很多次，而且有相同的 learning rate，所以有比較高的機率可以到相同的極值。
	\item 另外，從上一現象可以發現：在這次的 data 裡面，其實可以只使用速度較快、運算資源不用太多的 SGD 來做 training ，也可以得到和 GD 相近的效果。\\
	而如果給定相同的時間用 GD 和 SGD 來做的話， SGD 可能可以比 GD 更早下降到極值，或是 SGD 可能會有比 GD 更好的效果。
\end{enumerate}
\end{figure}
\end{document}
